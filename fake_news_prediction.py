# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RNcR2PlPxTIUyfeC5nYNZ3FVJ1orMNB
"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
news_dataset = pd.read_csv('/content/gdrive/My Drive/train.csv')

"""# Importing the Dependencies"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
import io
import string
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
# %matplotlib inline

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

"""# Punctuations Removal"""

def cleanpunc(sentence):
    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sentence)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    return  cleaned

"""# Data Pre-Processing"""

news_dataset.describe()

news_dataset.shape

# print the first 5 rows of the dataframe
news_dataset.head()

# counting the number of missing values in the dataset
news_dataset.isnull().sum()

news_dataset['label'].size

news_dataset['label_num'] = news_dataset['label'].map(lambda x: 1 if x == 1 else 0)

# replacing the null values with empty string
news_dataset = news_dataset.fillna('')

# merging the author name and news title
news_dataset['content'] = news_dataset['author']+' '+news_dataset['title']

print(news_dataset['content'])

# separating the data & label
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

"""# Data Cleaning"""

news_dataset.duplicated(subset={"title"}).value_counts()

# Deleting the duplicates
news_dataset_1 =  news_dataset.drop_duplicates(subset={"title"},keep="first")

news_dataset_1.duplicated(subset={"title"}).value_counts()

"""**Stemming** : is the process of reducing a word to its Root word

Example: actor, actress, acting --> act
"""

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

"""# Excluding important words for Sentiment Analysis"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

stop = stopwords.words('english') #All the stopwords in English language

#excluding some useful words from stop words list as we doing sentiment analysis
excluding = ['against','not','don', "don't",'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't",
             'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 
             'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't",'shouldn', "shouldn't", 'wasn',
             "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
stop = [words for words in stop if words not in excluding]
print(stop)

#initialising the Porter Stemmer
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()

"""# Preprocessing on the Dataset"""

i = 0
string1 = ' '
final_string = []
fake_words = []                
real_words = []
s = ''

for sent in news_dataset_1['text'].values:
    filtered_sentence = []

    for w in sent.split():
        if((w.isalpha()) and (len(w)>2)):  
            if(w.lower() not in stop):    # If it is a stopword
                s = (port_stem.stem(w.lower())).encode('utf8')
                filtered_sentence.append(s)
                if(news_dataset_1['label'].values)[i] == 0:
                    real_words.append(s)
                if(news_dataset_1['label'].values)[i] == 1:
                    fake_words.append(s)
            else:
                continue
        else:
            continue 
    string1 = b" ".join(filtered_sentence) 
    final_string.append(string1)
    i += 1

"""# Counting Fake and Real Words in Text"""

from collections import Counter
print("Number of Real words: ", len(real_words))
print("Number of Fake words: ", len(fake_words))

"""# Separating Data and Labels"""

#separating the data and label
X = news_dataset['content'].values
Y = news_dataset['label'].values

print(X)

print(Y)

Y.shape

# converting the textual data to numerical data
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

"""# Splitting the dataset to training and test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

"""# Logistic Regression"""

model_1 = LogisticRegression()
model_1.fit(X_train, Y_train)

"""# Passive Aggressive Classifier"""

model_2 = PassiveAggressiveClassifier()
model_2.fit(X_train, Y_train)

"""# Naive Bayes"""

model_3 = MultinomialNB()
model_3.fit(X_train, Y_train)

"""# Decision Tree Classifier"""

model_4 = DecisionTreeClassifier()
model_4.fit(X_train, Y_train)

"""# KNN"""

model_5 = KNeighborsClassifier(n_neighbors=3)
model_5.fit(X_train, Y_train)
pred_knn = model_5.predict(X_test)

"""# Evaluation

Model 1 : Logistic Regression
"""

# Accuracy score on the Training and Test Data
X_train_prediction = model_1.predict(X_train)
X_test_prediction = model_1.predict(X_test)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the testing  data : ', test_data_accuracy)

""" Model 2 : Passive Aggressive Classifier"""

# Accuracy score on the Training and Test Data
X_train_prediction = model_2.predict(X_train)
X_test_prediction = model_2.predict(X_test)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the testing  data : ', test_data_accuracy)

""" Model 3 : Naive Bayes"""

# Accuracy score on the Training and Test Data
X_train_prediction = model_3.predict(X_train)
X_test_prediction = model_3.predict(X_test)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the testing  data : ', test_data_accuracy)

"""Model 4 : Decision Tree Classifier"""

# Accuracy score on the Training and Test Data
X_train_prediction = model_4.predict(X_train)
X_test_prediction = model_4.predict(X_test)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the testing  data : ', test_data_accuracy)

"""Model 5 : KNN"""

# Accuracy score on the Training and Test Data
X_train_prediction = model_5.predict(X_train)
X_test_prediction = model_5.predict(X_test)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the training data : ', training_data_accuracy)
print('Accuracy score of the testing  data : ', test_data_accuracy)

"""# Predictive System

Test Case 1 : Testing for Real News
"""

X_new = X_test[1]

prediction = model_1.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

print(Y_test[1])

"""Test Case 2 : Testing for Fake News"""

X_new = X_test[2]

prediction = model_1.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

print(Y_test[2])

"""# Graphical Analysis for Training Dataset"""

model_accuracy = {
    'Logistic Regression': 0.986,
    'Passive Aggressive Classifier': 1.0,
    'Naive Bayes': 0.978,
    'Decision Tree Classifier': 1.0,
    'KNN': 0.586
}

pd.Series(model_accuracy).plot(kind='pie',label="")
plt.title('Accuracy Score on Training Dataset',fontsize=20)

"""# Graphical Analysis for Testing Dataset"""

model_accuracy = {
    'Logistic Regression': 0.979,
    'Passive Aggressive Classifier': 0.9918,
    'Naive Bayes': 0.955,
    'Decision Tree Classifier': 0.991,
    'KNN': 0.536
}

pd.Series(model_accuracy).plot(kind='pie',label="")
plt.title('Accuracy Score on Testing Dataset',fontsize=20)